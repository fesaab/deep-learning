{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis with TFLearn\n",
    "\n",
    "In this notebook, we'll continue Andrew Trask's work by building a network for sentiment analysis on the movie review data. Instead of a network written with Numpy, we'll be using [TFLearn](http://tflearn.org/), a high-level library built on top of TensorFlow. TFLearn makes it simpler to build networks just by defining the layers. It takes care of most of the details for you.\n",
    "\n",
    "We'll start off by importing all the modules we'll need, then load and prepare the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tflearn\n",
    "from tflearn.data_utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data\n",
    "\n",
    "Following along with Andrew, our goal here is to convert our reviews into word vectors. The word vectors will have elements representing words in the total vocabulary. If the second position represents the word 'the', for each review we'll count up the number of times 'the' appears in the text and set the second position to that count. I'll show you examples as we build the input data from the reviews data. Check out Andrew's notebook and video for more about this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the data\n",
    "\n",
    "Use the pandas library to read the reviews and postive/negative labels from comma-separated files. The data we're using has already been preprocessed a bit and we know it uses only lower case characters. If we were working from raw data, where we didn't know it was all lower case, we would want to add a step here to convert it. That's so we treat different variations of the same word, like `The`, `the`, and `THE`, all the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reviews = pd.read_csv('reviews.txt', header=None)\n",
    "labels = pd.read_csv('labels.txt', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 'bromwell high is a cartoon comedy . it ran at the same time as some other programs about school life  such as  teachers  . my   years in the teaching profession lead me to believe that bromwell high  s satire is much closer to reality than is  teachers  . the scramble to survive financially  the insightful students who can see right through their pathetic teachers  pomp  the pettiness of the whole situation  all remind me of the schools i knew and their students . when i saw the episode in which a student repeatedly tried to burn down the school  i immediately recalled . . . . . . . . . at . . . . . . . . . . high . a classic line inspector i  m here to sack one of your teachers . student welcome to bromwell high . i expect that many adults of my age think that bromwell high is far fetched . what a pity that it isn  t   '],\n",
       "       [ 'story of a man who has unnatural feelings for a pig . starts out with a opening scene that is a terrific example of absurd comedy . a formal orchestra audience is turned into an insane  violent mob by the crazy chantings of it  s singers . unfortunately it stays absurd the whole time with no general narrative eventually making it just too off putting . even those from the era should be turned off . the cryptic dialogue would make shakespeare seem easy to a third grader . on a technical level it  s better than you might think with some good cinematography by future great vilmos zsigmond . future stars sally kirkland and frederic forrest can be seen briefly .  '],\n",
       "       [ 'homelessness  or houselessness as george carlin stated  has been an issue for years but never a plan to help those on the street that were once considered human who did everything from going to school  work  or vote for the matter . most people think of the homeless as just a lost cause while worrying about things such as racism  the war on iraq  pressuring kids to succeed  technology  the elections  inflation  or worrying if they  ll be next to end up on the streets .  br    br   but what if you were given a bet to live on the streets for a month without the luxuries you once had from a home  the entertainment sets  a bathroom  pictures on the wall  a computer  and everything you once treasure to see what it  s like to be homeless  that is goddard bolt  s lesson .  br    br   mel brooks  who directs  who stars as bolt plays a rich man who has everything in the world until deciding to make a bet with a sissy rival  jeffery tambor  to see if he can live in the streets for thirty days without the luxuries if bolt succeeds  he can do what he wants with a future project of making more buildings . the bet  s on where bolt is thrown on the street with a bracelet on his leg to monitor his every move where he can  t step off the sidewalk . he  s given the nickname pepto by a vagrant after it  s written on his forehead where bolt meets other characters including a woman by the name of molly  lesley ann warren  an ex  dancer who got divorce before losing her home  and her pals sailor  howard morris  and fumes  teddy wilson  who are already used to the streets . they  re survivors . bolt isn  t . he  s not used to reaching mutual agreements like he once did when being rich where it  s fight or flight  kill or be killed .  br    br   while the love connection between molly and bolt wasn  t necessary to plot  i found  life stinks  to be one of mel brooks  observant films where prior to being a comedy  it shows a tender side compared to his slapstick work such as blazing saddles  young frankenstein  or spaceballs for the matter  to show what it  s like having something valuable before losing it the next day or on the other hand making a stupid bet like all rich people do when they don  t know what to do with their money . maybe they should give it to the homeless instead of using it like monopoly money .  br    br   or maybe this film will inspire you to help others .  '],\n",
       "       ..., \n",
       "       [ 'some films that you pick up for a pound turn out to be rather good    rd century films released dozens of obscure italian and american movie that were great  but although hardgore released some fulci films amongst others  the bulk of their output is crap like the zombie chronicles .  br    br   the only positive thing i can say about this film is that it  s nowhere near as annoying as the stink of flesh . other than that  its a very clumsy anthology film with the technical competence of a lego house built by a whelk .  br    br   it  s been noted elsewhere  but you really do have to worry about a film that inserts previews of the action into its credit sequence  so by the time it gets to the zombie attacks  you  ve seen it all already .  br    br   bad movie fans will have a ball watching the       continuity mistakes and the diabolical acting of the cast  especially the hitchhiker  who was so bad he did make me laugh a bit   and kudos to hardgore for getting in to the spirit of things by releasing a print so bad it felt like i was watching some beat up home video of a camping trip .  br    br   awful  awful stuff . we  ve all made stuff like this when we  ve gotten a hold of a camera  but common sense prevails and these films languish in our cupboards somewhere . avoid .  '],\n",
       "       [ 'working  class romantic drama from director martin ritt is as unbelievable as they come  yet there are moments of pleasure due mostly to the charisma of stars jane fonda and robert de niro  both terrific  . she  s a widow who can  t move on  he  s illiterate and a closet  inventor   you can probably guess the rest . adaptation of pat barker  s novel  union street   a better title   is so laid  back it verges on bland  and the film  s editing is a mess  but it  s still pleasant a rosy  hued blue  collar fantasy . there are no overtures to serious issues  even the illiteracy angle is just a plot  tool for the ensuing love story  and no real fireworks  though the characters are intentionally a bit colorless and the leads are toned down to an interesting degree . the finale is pure fluff   and cynics will find it difficult to swallow   though these two characters deserve a happy ending and the picture wouldn  t really be satisfying any other way .  from   '],\n",
       "       [ 'this is one of the dumbest films  i  ve ever seen . it rips off nearly ever type of thriller and manages to make a mess of them all .  br    br   there  s not a single good line or character in the whole mess . if there was a plot  it was an afterthought and as far as acting goes  there  s nothing good to say so ill say nothing . i honestly cant understand how this type of nonsense gets produced and actually released  does somebody somewhere not at some stage think   oh my god this really is a load of shite  and call it a day . its crap like this that has people downloading illegally  the trailer looks like a completely different film  at least if you have download it  you haven  t wasted your time or money don  t waste your time  this is painful .  ']], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting word frequency\n",
    "\n",
    "To start off we'll need to count how often each word appears in the data. We'll use this count to create a vocabulary we'll use to encode the review data. This resulting count is known as a [bag of words](https://en.wikipedia.org/wiki/Bag-of-words_model). We'll use it to select our vocabulary and build the word vectors. You should have seen how to do this in Andrew's lesson. Try to implement it here using the [Counter class](https://docs.python.org/2/library/collections.html#collections.Counter).\n",
    "\n",
    "> **Exercise:** Create the bag of words from the reviews data and assign it to `total_counts`. The reviews are stores in the `reviews` [Pandas DataFrame](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html). If you want the reviews as a Numpy array, use `reviews.values`. You can iterate through the rows in the DataFrame with `for idx, row in reviews.iterrows():` ([documentation](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.iterrows.html)). When you break up the reviews into words, use `.split(' ')` instead of `.split()` so your results match ours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words in data set:  74074\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "total_counts = Counter()\n",
    "\n",
    "for review in reviews.values:\n",
    "    total_counts.update(review[0].split(' '))\n",
    "\n",
    "print(\"Total words in data set: \", len(total_counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's keep the first 10000 most frequent words. As Andrew noted, most of the words in the vocabulary are rarely used so they will have little effect on our predictions. Below, we'll sort `vocab` by the count value and keep the 10000 most frequent words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'the', '.', 'and', 'a', 'of', 'to', 'is', 'br', 'it', 'in', 'i', 'this', 'that', 's', 'was', 'as', 'for', 'with', 'movie', 'but', 'film', 'you', 'on', 't', 'not', 'he', 'are', 'his', 'have', 'be', 'one', 'all', 'at', 'they', 'by', 'an', 'who', 'so', 'from', 'like', 'there', 'her', 'or', 'just', 'about', 'out', 'if', 'has', 'what', 'some', 'good', 'can', 'more', 'she', 'when', 'very', 'up', 'time', 'no']\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(total_counts, key=total_counts.get, reverse=True)[:10000]\n",
    "#vocab = sorted(total_counts, key=total_counts.get, reverse=True)\n",
    "print(vocab[:60])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's the last word in our vocabulary? We can use this to judge if 10000 is too few. If the last word is pretty common, we probably need to keep more words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defines :  30\n"
     ]
    }
   ],
   "source": [
    "print(vocab[-1], ': ', total_counts[vocab[-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last word in our vocabulary shows up 30 times in 25000 reviews. I think it's fair to say this is a tiny proportion. We are probably fine with this number of words.\n",
    "\n",
    "**Note:** When you run, you may see a different word from the one shown above, but it will also have the value `30`. That's because there are many words tied for that number of counts, and the `Counter` class does not guarantee which one will be returned in the case of a tie.\n",
    "\n",
    "Now for each review in the data, we'll make a word vector. First we need to make a mapping of word to index, pretty easy to do with a dictionary comprehension.\n",
    "\n",
    "> **Exercise:** Create a dictionary called `word2idx` that maps each word in the vocabulary to an index. The first word in `vocab` has index `0`, the second word has index `1`, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'': 0,\n",
       " 'roles': 546,\n",
       " 'lovers': 1799,\n",
       " 'account': 2625,\n",
       " 'confusing': 1478,\n",
       " 'sister': 737,\n",
       " 'rented': 1591,\n",
       " 'conditions': 5074,\n",
       " 'oriental': 9409,\n",
       " 'salman': 5998,\n",
       " 'platoon': 9122,\n",
       " 'fellini': 7151,\n",
       " 'gotten': 1826,\n",
       " 'expand': 8985,\n",
       " 'audience': 301,\n",
       " 'hello': 4650,\n",
       " 'tracy': 2885,\n",
       " 'fifties': 5538,\n",
       " 'location': 1593,\n",
       " 'integrity': 5008,\n",
       " 'balcony': 9807,\n",
       " 'planets': 9575,\n",
       " 'gino': 7556,\n",
       " 'gooding': 6794,\n",
       " 'fortune': 3179,\n",
       " 'arizona': 9217,\n",
       " 'african': 2024,\n",
       " 'stevens': 4464,\n",
       " 'yellow': 4072,\n",
       " 'recall': 2248,\n",
       " 'absolute': 1535,\n",
       " 'showed': 1157,\n",
       " 'ally': 6151,\n",
       " 'parts': 522,\n",
       " 'coffin': 7007,\n",
       " 'toss': 8783,\n",
       " 'afraid': 1569,\n",
       " 'letter': 3282,\n",
       " 'outing': 5499,\n",
       " 'grabbed': 7126,\n",
       " 'unclear': 7346,\n",
       " 'magician': 6235,\n",
       " 'beloved': 2707,\n",
       " 'green': 1372,\n",
       " 'desires': 5149,\n",
       " 'claire': 2786,\n",
       " 'glory': 3140,\n",
       " 'flying': 1521,\n",
       " 'arnold': 3177,\n",
       " 'knightley': 5820,\n",
       " 'malden': 8297,\n",
       " 'anthony': 1965,\n",
       " 'throwing': 2783,\n",
       " 'allow': 1713,\n",
       " 'eastwood': 3296,\n",
       " 'error': 6033,\n",
       " 'positives': 9925,\n",
       " 'teacher': 1692,\n",
       " 'rejects': 7029,\n",
       " 'shorts': 3121,\n",
       " 'laced': 8578,\n",
       " 'eventual': 6312,\n",
       " 'blah': 2493,\n",
       " 'drowning': 7717,\n",
       " 'related': 2445,\n",
       " 'elliott': 6706,\n",
       " 'mouths': 6721,\n",
       " 'types': 2083,\n",
       " 'facial': 2724,\n",
       " 'scattered': 8713,\n",
       " 'sent': 1398,\n",
       " 'crazy': 902,\n",
       " 'employed': 5588,\n",
       " 'furniture': 6536,\n",
       " 'makes': 167,\n",
       " 'reasonable': 3761,\n",
       " 'knew': 687,\n",
       " 'protective': 7623,\n",
       " 'detailed': 3991,\n",
       " 'yeti': 5362,\n",
       " 'bauer': 7240,\n",
       " 'accurately': 5860,\n",
       " 'wrenching': 6060,\n",
       " 'systems': 8188,\n",
       " 'heavy': 1148,\n",
       " 'paint': 2530,\n",
       " 'rub': 9602,\n",
       " 'quite': 180,\n",
       " 'band': 1081,\n",
       " 'closet': 4147,\n",
       " 'abundance': 7797,\n",
       " 'instruments': 9922,\n",
       " 'suspended': 9239,\n",
       " 'notable': 2838,\n",
       " 'always': 209,\n",
       " 'unoriginal': 4920,\n",
       " 'adaptation': 1232,\n",
       " 'famed': 7499,\n",
       " 'garcia': 9195,\n",
       " 'schools': 5829,\n",
       " 'you': 22,\n",
       " 'direct': 1480,\n",
       " 'sordid': 9935,\n",
       " 'sixteen': 7819,\n",
       " 'khouri': 8929,\n",
       " 'cohesive': 8061,\n",
       " 'beau': 9489,\n",
       " 'insult': 2346,\n",
       " 'beard': 7397,\n",
       " 'angelina': 7331,\n",
       " 'lead': 476,\n",
       " 'driving': 1924,\n",
       " 'vera': 6942,\n",
       " 'clarence': 8105,\n",
       " 'walls': 3372,\n",
       " 'marred': 9172,\n",
       " 'pure': 1032,\n",
       " 'crossing': 6051,\n",
       " 'lol': 3976,\n",
       " 'farce': 3607,\n",
       " 'min': 4590,\n",
       " 'produces': 6917,\n",
       " 'inevitably': 5014,\n",
       " 'male': 887,\n",
       " 'motorcycle': 6845,\n",
       " 'weaker': 6064,\n",
       " 'dylan': 5704,\n",
       " 'yearning': 9049,\n",
       " 'worth': 289,\n",
       " 'predecessor': 5920,\n",
       " 'mj': 9075,\n",
       " 'develops': 3181,\n",
       " 'wasn': 286,\n",
       " 'emma': 2439,\n",
       " 'mattei': 8655,\n",
       " 'realm': 5450,\n",
       " 'cannibal': 4911,\n",
       " 'stunts': 3275,\n",
       " 'inhabited': 9010,\n",
       " 'promote': 5554,\n",
       " 'loneliness': 5250,\n",
       " 'granger': 8508,\n",
       " 'suspenseful': 2531,\n",
       " 'brush': 8111,\n",
       " 'galactica': 7008,\n",
       " 'avoid': 781,\n",
       " 'complained': 7890,\n",
       " 'creatures': 2326,\n",
       " 'dies': 1421,\n",
       " 'senior': 6300,\n",
       " 'suave': 7947,\n",
       " 'glossy': 7577,\n",
       " 'subplot': 3642,\n",
       " 'horrible': 518,\n",
       " 'capshaw': 9697,\n",
       " 'joel': 5030,\n",
       " 'bash': 9174,\n",
       " 'destroying': 4485,\n",
       " 'apparently': 672,\n",
       " 'commenting': 5871,\n",
       " 'villagers': 9164,\n",
       " 'krueger': 7632,\n",
       " 'torment': 8072,\n",
       " 'flee': 9552,\n",
       " 'seattle': 7624,\n",
       " 'unforgettable': 3194,\n",
       " 'comics': 3378,\n",
       " 'client': 8028,\n",
       " 'ultra': 3261,\n",
       " 'daughter': 539,\n",
       " 'gilbert': 6609,\n",
       " 'tepid': 9601,\n",
       " 'virtue': 8382,\n",
       " 'finest': 1865,\n",
       " 'paz': 7991,\n",
       " 'prefers': 8805,\n",
       " 'meal': 6851,\n",
       " 'german': 1101,\n",
       " 'lions': 8359,\n",
       " 'kerr': 9458,\n",
       " 'monkey': 3390,\n",
       " 'tim': 1712,\n",
       " 'normally': 1734,\n",
       " 'kate': 1747,\n",
       " 'exhilarating': 9926,\n",
       " 'talks': 2292,\n",
       " 'shaggy': 7005,\n",
       " 'drove': 5558,\n",
       " 'foolish': 6299,\n",
       " 'subtitled': 8947,\n",
       " 'soup': 5540,\n",
       " 'laurie': 6288,\n",
       " 'fable': 8882,\n",
       " 'ram': 7596,\n",
       " 'joan': 1745,\n",
       " 'clunky': 7463,\n",
       " 'come': 215,\n",
       " 'owen': 4754,\n",
       " 'archie': 9904,\n",
       " 'along': 365,\n",
       " 'flawless': 3540,\n",
       " 'manipulate': 8770,\n",
       " 'helping': 2734,\n",
       " 'nightmare': 1676,\n",
       " 'galaxy': 6147,\n",
       " 'steaming': 8582,\n",
       " 'timed': 8159,\n",
       " 'commented': 4264,\n",
       " 'promoted': 6404,\n",
       " 'natalie': 5888,\n",
       " 'intensely': 7076,\n",
       " 'discovers': 2133,\n",
       " 'scenario': 2650,\n",
       " 'shake': 4432,\n",
       " 'encourages': 8828,\n",
       " 'declares': 9777,\n",
       " 'depicting': 4985,\n",
       " 'sub': 1465,\n",
       " 'shrill': 8784,\n",
       " 'lethal': 6615,\n",
       " 'intricate': 6484,\n",
       " 'pursues': 9778,\n",
       " '.': 2,\n",
       " 'when': 55,\n",
       " 'symbols': 8122,\n",
       " 'incident': 3803,\n",
       " 'steps': 3153,\n",
       " 'abomination': 7948,\n",
       " 'ounce': 9740,\n",
       " 'snl': 4383,\n",
       " 'scenes': 138,\n",
       " 'force': 1106,\n",
       " 'being': 112,\n",
       " 'spoofs': 8112,\n",
       " 'fascist': 6675,\n",
       " 'goldsworthy': 5890,\n",
       " 'distinctive': 8073,\n",
       " 'hurt': 1433,\n",
       " 'second': 331,\n",
       " 'recurring': 8833,\n",
       " 'holiday': 3122,\n",
       " 'stumbling': 8992,\n",
       " 'rotting': 8916,\n",
       " 'educational': 4947,\n",
       " 'escaping': 6421,\n",
       " 'cohen': 5660,\n",
       " 'hogan': 8657,\n",
       " 'program': 2035,\n",
       " 'rich': 997,\n",
       " 'stealth': 9808,\n",
       " 'seeing': 317,\n",
       " 'apart': 952,\n",
       " 'food': 1609,\n",
       " 'romano': 9745,\n",
       " 'posey': 5254,\n",
       " 'clue': 2261,\n",
       " 'dying': 1693,\n",
       " 'exposure': 4994,\n",
       " 'ewan': 9605,\n",
       " 'poker': 5975,\n",
       " 'hardest': 9178,\n",
       " 'joshua': 8298,\n",
       " 'shape': 2968,\n",
       " 'gypo': 6233,\n",
       " 'rko': 8789,\n",
       " 'barbarian': 8873,\n",
       " 'driven': 2137,\n",
       " 'desperate': 1645,\n",
       " 'kumar': 5083,\n",
       " 'michel': 9398,\n",
       " 'want': 182,\n",
       " 'attempted': 3335,\n",
       " 'telling': 966,\n",
       " 'legitimate': 6820,\n",
       " 'bridges': 5779,\n",
       " 'basket': 6422,\n",
       " 'ratio': 8625,\n",
       " 'mainstream': 2448,\n",
       " 'vincenzo': 7977,\n",
       " 'concept': 1096,\n",
       " 'forrest': 7996,\n",
       " 'trials': 5903,\n",
       " 'proving': 5854,\n",
       " 'unfortunately': 465,\n",
       " 'realist': 9631,\n",
       " 'ps': 5142,\n",
       " 'casted': 8200,\n",
       " 'climactic': 4522,\n",
       " 'attacking': 6475,\n",
       " 'countless': 3336,\n",
       " 'queen': 1557,\n",
       " 'gene': 1869,\n",
       " 'tasty': 8378,\n",
       " 'confronting': 9123,\n",
       " 'review': 720,\n",
       " 'sentenced': 9465,\n",
       " 'kicked': 4322,\n",
       " 'guessing': 3077,\n",
       " 'drags': 3430,\n",
       " 'stephanie': 6468,\n",
       " 'cos': 9781,\n",
       " 'sanity': 7515,\n",
       " 'insulting': 3466,\n",
       " 'fate': 1923,\n",
       " 'trained': 4368,\n",
       " 'insisted': 9698,\n",
       " 'bartender': 7975,\n",
       " 'honorable': 9152,\n",
       " 'however': 190,\n",
       " 'boyle': 5122,\n",
       " 'engineer': 7837,\n",
       " 'sandler': 2770,\n",
       " 'hunt': 2272,\n",
       " 'remind': 3002,\n",
       " 'spotlight': 7498,\n",
       " 'miracles': 9493,\n",
       " 'filmmaking': 6409,\n",
       " 'wet': 4394,\n",
       " 'mob': 3003,\n",
       " 'poor': 336,\n",
       " 'wtc': 9753,\n",
       " 'worldwide': 9043,\n",
       " 'murder': 571,\n",
       " 'community': 1804,\n",
       " 'casual': 5645,\n",
       " 'writing': 479,\n",
       " 'wearing': 1637,\n",
       " 'faults': 4377,\n",
       " 'sacrificing': 9942,\n",
       " 'throne': 8374,\n",
       " 'practicing': 9603,\n",
       " 'formidable': 9955,\n",
       " 'does': 127,\n",
       " 'grumpy': 9451,\n",
       " 'missile': 6156,\n",
       " 'replacement': 7213,\n",
       " 'depiction': 2797,\n",
       " 'brenda': 4898,\n",
       " 'ocean': 4161,\n",
       " 'spinal': 5644,\n",
       " 'ruby': 4422,\n",
       " 'involving': 1209,\n",
       " 'vignettes': 7308,\n",
       " 'insert': 7521,\n",
       " 'bombing': 8999,\n",
       " 'barrel': 5474,\n",
       " 'sarcasm': 7019,\n",
       " 'farrell': 4675,\n",
       " 'alex': 2201,\n",
       " 'wish': 640,\n",
       " 'meg': 5447,\n",
       " 'winters': 4105,\n",
       " 'causing': 4045,\n",
       " 'endings': 4126,\n",
       " 'none': 590,\n",
       " 'sour': 7398,\n",
       " 'sympathetic': 2212,\n",
       " 'indian': 1361,\n",
       " 'buffs': 4323,\n",
       " 'video': 367,\n",
       " 'reflect': 4562,\n",
       " 'abc': 3534,\n",
       " 'peace': 2402,\n",
       " 'chills': 5383,\n",
       " 'physical': 1720,\n",
       " 'mexican': 2645,\n",
       " 'furious': 4957,\n",
       " 'actresses': 1472,\n",
       " 'macho': 5549,\n",
       " 'faye': 7759,\n",
       " 'ambiguous': 5139,\n",
       " 'banter': 6105,\n",
       " 'bank': 1957,\n",
       " 'phantom': 4676,\n",
       " 'gold': 1782,\n",
       " 'u': 1165,\n",
       " 'spain': 4836,\n",
       " 'risk': 2919,\n",
       " 'feared': 9092,\n",
       " 'trent': 9820,\n",
       " 'fleeing': 8966,\n",
       " 'hostage': 7373,\n",
       " 'concerned': 1930,\n",
       " 'actually': 166,\n",
       " 'connolly': 9070,\n",
       " 'www': 5530,\n",
       " 'become': 410,\n",
       " 'shockingly': 6809,\n",
       " 'underlying': 4983,\n",
       " 'accepting': 6485,\n",
       " 'mothers': 5658,\n",
       " 'voices': 2302,\n",
       " 'betrays': 9454,\n",
       " 'contacts': 9771,\n",
       " 'icons': 8652,\n",
       " 'explaining': 4037,\n",
       " 'caught': 1038,\n",
       " 'einstein': 5922,\n",
       " 'ironic': 2921,\n",
       " 'leaving': 1180,\n",
       " 'behalf': 9002,\n",
       " 'describing': 5979,\n",
       " 'firstly': 4581,\n",
       " 'confess': 5042,\n",
       " 'matches': 4234,\n",
       " 'presents': 2392,\n",
       " 'ins': 7322,\n",
       " 'c': 1014,\n",
       " 'james': 567,\n",
       " 'gypsy': 7695,\n",
       " 'somewhere': 1176,\n",
       " 'warning': 1699,\n",
       " 'terrified': 5572,\n",
       " 'treating': 9044,\n",
       " 'framing': 8091,\n",
       " 'memorable': 888,\n",
       " 'australian': 2406,\n",
       " 'leslie': 2779,\n",
       " 'perceive': 9169,\n",
       " 'mark': 914,\n",
       " 'directorial': 3663,\n",
       " 'cassavetes': 5585,\n",
       " 'shooting': 1183,\n",
       " 'belly': 6322,\n",
       " 'conscience': 5277,\n",
       " 'tourist': 6537,\n",
       " 'exterior': 6249,\n",
       " 'from': 39,\n",
       " 'unnamed': 9741,\n",
       " 'crowe': 7838,\n",
       " 'marketing': 4897,\n",
       " 'disgrace': 6046,\n",
       " 'witless': 9472,\n",
       " 'pub': 8587,\n",
       " 'bites': 8113,\n",
       " 'crisis': 3061,\n",
       " 'kennedy': 4046,\n",
       " 'convey': 2807,\n",
       " 'hopefully': 2327,\n",
       " 'rifle': 5222,\n",
       " 'mick': 7043,\n",
       " 'line': 342,\n",
       " 'answering': 9809,\n",
       " 'exceedingly': 9906,\n",
       " 'difference': 1449,\n",
       " 'mamet': 7722,\n",
       " 'muppet': 5410,\n",
       " 'kentucky': 9380,\n",
       " 'racism': 3044,\n",
       " 'canceled': 6656,\n",
       " 'identities': 8558,\n",
       " 'dilemma': 6323,\n",
       " 'poke': 9006,\n",
       " 'munchies': 9438,\n",
       " 'buys': 6250,\n",
       " 'crashed': 8022,\n",
       " 'duck': 5661,\n",
       " 'unemployed': 9103,\n",
       " 'egypt': 7915,\n",
       " 'threw': 3888,\n",
       " 'aircraft': 8399,\n",
       " 'bust': 7485,\n",
       " 'cameron': 3646,\n",
       " 'gender': 4623,\n",
       " 'spine': 6155,\n",
       " 'canada': 3268,\n",
       " 's': 14,\n",
       " 'tip': 5541,\n",
       " 'implausible': 4012,\n",
       " 'notices': 7997,\n",
       " 'christ': 2668,\n",
       " 'redemption': 3201,\n",
       " 'occur': 3890,\n",
       " 'certain': 793,\n",
       " 'liberal': 3649,\n",
       " 'near': 735,\n",
       " 'simplicity': 4591,\n",
       " 'uptight': 8468,\n",
       " 'richly': 8486,\n",
       " 'characteristics': 6351,\n",
       " 'belongs': 3221,\n",
       " 'ripping': 5948,\n",
       " 'forgotten': 1532,\n",
       " 'budding': 8174,\n",
       " 'photographs': 6088,\n",
       " 'rambling': 7143,\n",
       " 'tax': 6315,\n",
       " 'luckily': 3431,\n",
       " 'toro': 8332,\n",
       " 'outrageous': 3556,\n",
       " 'balls': 4552,\n",
       " 'dwarf': 8299,\n",
       " 'appreciate': 1124,\n",
       " 'welcomed': 9097,\n",
       " 'functions': 8394,\n",
       " 'dread': 6168,\n",
       " 'undoubtedly': 4025,\n",
       " 'event': 1461,\n",
       " 'harmony': 9578,\n",
       " 'respect': 1137,\n",
       " 'october': 7218,\n",
       " 'camping': 7688,\n",
       " 'traditions': 7606,\n",
       " 'bean': 7579,\n",
       " 'despair': 4800,\n",
       " 'olsen': 7337,\n",
       " 'interpretations': 7435,\n",
       " 'bath': 4553,\n",
       " 'doses': 8826,\n",
       " 'neck': 3238,\n",
       " 'required': 2565,\n",
       " 'military': 1213,\n",
       " 'threatens': 5522,\n",
       " 'sequels': 2264,\n",
       " 'targets': 6913,\n",
       " 'times': 210,\n",
       " 'thats': 1441,\n",
       " 'large': 1035,\n",
       " 'heaven': 1665,\n",
       " 'achieves': 6705,\n",
       " 'wanna': 2976,\n",
       " 'puppets': 6089,\n",
       " 'washington': 1907,\n",
       " 'recent': 1115,\n",
       " 'twisted': 2476,\n",
       " 'ethan': 4541,\n",
       " 'whose': 616,\n",
       " 'turning': 1566,\n",
       " 'wonderland': 6169,\n",
       " 'order': 648,\n",
       " 'rear': 5949,\n",
       " 'anderson': 2280,\n",
       " 'one': 31,\n",
       " 'nightmarish': 7282,\n",
       " 'assure': 7374,\n",
       " 'competing': 9779,\n",
       " 'hayworth': 6402,\n",
       " 'jam': 8230,\n",
       " 'refuses': 3028,\n",
       " 'lord': 1548,\n",
       " 'brennan': 8470,\n",
       " 'internal': 6068,\n",
       " 'surviving': 4142,\n",
       " 'controlling': 6807,\n",
       " 'toys': 4047,\n",
       " 'proper': 2213,\n",
       " 'abuse': 2537,\n",
       " 'fi': 895,\n",
       " 'obscure': 3694,\n",
       " 'marx': 8750,\n",
       " 'lasts': 6159,\n",
       " 'mitch': 5858,\n",
       " 'elements': 777,\n",
       " 'puts': 1434,\n",
       " 'heat': 3472,\n",
       " 'race': 1491,\n",
       " 'harbor': 8350,\n",
       " 'robbins': 5089,\n",
       " 'arrived': 4523,\n",
       " 'level': 637,\n",
       " 'stranger': 3014,\n",
       " 'blacks': 5347,\n",
       " 'profession': 5878,\n",
       " 'obligatory': 5604,\n",
       " 'exact': 2569,\n",
       " 'california': 2579,\n",
       " 'coma': 7090,\n",
       " 'interested': 909,\n",
       " 'concludes': 9104,\n",
       " 'depths': 6306,\n",
       " 'invited': 5486,\n",
       " 'imagining': 7273,\n",
       " 'python': 6766,\n",
       " 'happenings': 7964,\n",
       " 'locks': 9440,\n",
       " 'man': 126,\n",
       " 'regular': 1948,\n",
       " 'dillon': 8056,\n",
       " 'emotion': 1393,\n",
       " 'tomlinson': 9990,\n",
       " 'cube': 3139,\n",
       " 'skilled': 6778,\n",
       " 'placement': 8193,\n",
       " 'mexico': 2627,\n",
       " 'shaky': 5051,\n",
       " 'granted': 2450,\n",
       " 'pianist': 9918,\n",
       " 'wolf': 3670,\n",
       " 'some': 50,\n",
       " 'stalks': 8993,\n",
       " 'minor': 1378,\n",
       " 'elevator': 5966,\n",
       " 'tokyo': 6066,\n",
       " 'cameras': 3917,\n",
       " 'mayhem': 5478,\n",
       " 'aspirations': 9872,\n",
       " 'ranger': 8718,\n",
       " 'psychiatrist': 3614,\n",
       " 'pants': 4466,\n",
       " 'mourning': 9323,\n",
       " 'son': 464,\n",
       " 'limitations': 6130,\n",
       " 'shock': 1423,\n",
       " 'managed': 1297,\n",
       " 'gets': 212,\n",
       " 'revelations': 9196,\n",
       " 'elvira': 3047,\n",
       " 'psychological': 1952,\n",
       " 'defense': 4781,\n",
       " 'torturing': 8636,\n",
       " 'really': 66,\n",
       " 'factor': 2303,\n",
       " 'mini': 2338,\n",
       " 'suffers': 2453,\n",
       " 'alexander': 3612,\n",
       " 'eagerly': 7095,\n",
       " 'mixing': 6012,\n",
       " 'exists': 2939,\n",
       " 'torch': 8810,\n",
       " 'entertained': 2150,\n",
       " 'malone': 5078,\n",
       " 'ignore': 2725,\n",
       " 'room': 655,\n",
       " 'kriemhild': 9271,\n",
       " 'napoleon': 9447,\n",
       " 'update': 6703,\n",
       " 'noting': 8009,\n",
       " 'launch': 7164,\n",
       " 'st': 1443,\n",
       " 'theories': 6550,\n",
       " 'darkly': 7998,\n",
       " 'seeming': 5912,\n",
       " 'underdog': 9349,\n",
       " 'choppy': 5298,\n",
       " 'badly': 890,\n",
       " 'best': 118,\n",
       " 'cope': 5555,\n",
       " 'theo': 8938,\n",
       " 'intellectually': 8583,\n",
       " 'buddies': 3954,\n",
       " 'masterfully': 6065,\n",
       " 'inferior': 4494,\n",
       " 'rapes': 8866,\n",
       " 'cbs': 5970,\n",
       " 'nevertheless': 2157,\n",
       " 'derivative': 6211,\n",
       " 'crass': 9062,\n",
       " 'etc': 515,\n",
       " 'lotr': 9502,\n",
       " 'freaks': 5366,\n",
       " 'series': 197,\n",
       " 'culkin': 8214,\n",
       " 'diver': 9427,\n",
       " 'obsessive': 6599,\n",
       " 'bacon': 5217,\n",
       " 'miserable': 4274,\n",
       " 'hopper': 4327,\n",
       " 'centers': 4603,\n",
       " 'door': 1285,\n",
       " 'climbs': 8968,\n",
       " 'warren': 3750,\n",
       " 'athletic': 8649,\n",
       " 'taker': 8706,\n",
       " 'minnelli': 6374,\n",
       " 'quaid': 5334,\n",
       " 'planes': 6810,\n",
       " 'cliched': 6801,\n",
       " 'officials': 8494,\n",
       " 'assumes': 7852,\n",
       " 'leader': 2072,\n",
       " 'egg': 7965,\n",
       " 'chong': 6749,\n",
       " 'sullavan': 9744,\n",
       " 'werner': 9325,\n",
       " 'miike': 3395,\n",
       " 'copied': 6504,\n",
       " 'donna': 3795,\n",
       " 'hometown': 7945,\n",
       " 'guide': 3543,\n",
       " 'lure': 8925,\n",
       " 'magnificent': 1982,\n",
       " 'section': 2393,\n",
       " 'casting': 950,\n",
       " 'bikini': 6553,\n",
       " 'said': 302,\n",
       " 'achieve': 2699,\n",
       " 'opposition': 8395,\n",
       " 'comes': 263,\n",
       " 'happy': 632,\n",
       " 'quasi': 7247,\n",
       " 'cab': 6997,\n",
       " 'mcqueen': 5942,\n",
       " 'kinky': 9175,\n",
       " 'east': 2814,\n",
       " 'effects': 299,\n",
       " 'corpse': 3539,\n",
       " 'performing': 3424,\n",
       " 'excruciating': 6977,\n",
       " 'realises': 7512,\n",
       " 'era': 965,\n",
       " 'controls': 7409,\n",
       " 'starving': 9866,\n",
       " 'moore': 2239,\n",
       " 'fingers': 5195,\n",
       " 'static': 5759,\n",
       " 'comfortable': 3936,\n",
       " 'danny': 1620,\n",
       " 'collaboration': 8456,\n",
       " 'curiosity': 3573,\n",
       " 'sassy': 5465,\n",
       " 'offered': 2538,\n",
       " 'offended': 4120,\n",
       " 'strung': 7473,\n",
       " 'principals': 6391,\n",
       " 'sleeps': 5962,\n",
       " 'desolate': 9507,\n",
       " 'poverty': 3388,\n",
       " 'nothing': 164,\n",
       " 'convoluted': 3655,\n",
       " 'dietrich': 8811,\n",
       " 'group': 586,\n",
       " 'wrote': 1017,\n",
       " 'orange': 4842,\n",
       " 'threat': 3781,\n",
       " 'root': 3615,\n",
       " 'cartoons': 2417,\n",
       " 'goat': 7243,\n",
       " 'hasn': 1463,\n",
       " 'attracted': 3595,\n",
       " 'interrupted': 7440,\n",
       " 'atrocity': 7980,\n",
       " 'avant': 9894,\n",
       " 'definitely': 403,\n",
       " 'neighbor': 3285,\n",
       " 'rotten': 4431,\n",
       " 'bucket': 9055,\n",
       " 'explains': 2522,\n",
       " 'jeff': 1757,\n",
       " 'cody': 6993,\n",
       " 'halloween': 2102,\n",
       " 'hmm': 7258,\n",
       " 'innovative': 3932,\n",
       " 'daniels': 5428,\n",
       " 'included': 1916,\n",
       " 'departure': 5815,\n",
       " 'commit': 3485,\n",
       " 'fleshed': 6586,\n",
       " 'greatness': 5029,\n",
       " 'strange': 667,\n",
       " 'suitably': 5617,\n",
       " 'lost': 408,\n",
       " 'earth': 664,\n",
       " 'jaws': 4763,\n",
       " 'hiding': 3188,\n",
       " 'doctors': 5688,\n",
       " 'adam': 1737,\n",
       " 'giant': 1437,\n",
       " 'goofy': 2942,\n",
       " 'rolls': 7360,\n",
       " 'simpsons': 7044,\n",
       " 'marrying': 8457,\n",
       " 'eliminated': 9441,\n",
       " 'adding': 2872,\n",
       " 'lola': 6324,\n",
       " 'sally': 3346,\n",
       " 'wrestler': 7513,\n",
       " 'likes': 1207,\n",
       " 'panic': 3768,\n",
       " 'ideals': 7802,\n",
       " 'accompany': 9986,\n",
       " 'mcbain': 9873,\n",
       " 'adaptations': 5090,\n",
       " 'metal': 2573,\n",
       " 'fantasies': 5278,\n",
       " 'must': 207,\n",
       " 'paid': 1516,\n",
       " 'exploration': 4546,\n",
       " 'tasteful': 9543,\n",
       " 'tube': 5134,\n",
       " 'ourselves': 3116,\n",
       " 'cheerful': 7666,\n",
       " 'artwork': 5992,\n",
       " 'gesture': 9644,\n",
       " 'postman': 9870,\n",
       " 'to': 6,\n",
       " 'dreyfuss': 7581,\n",
       " 'raunchy': 7692,\n",
       " 'involved': 563,\n",
       " 'humans': 1670,\n",
       " 'beverly': 5292,\n",
       " 'par': 2240,\n",
       " 'homosexual': 4635,\n",
       " 'friends': 360,\n",
       " 'choreography': 3818,\n",
       " 'research': 2274,\n",
       " 'jodie': 6342,\n",
       " 'hugely': 6117,\n",
       " 'studied': 7366,\n",
       " 'live': 409,\n",
       " 'oddly': 2958,\n",
       " 'evidently': 5480,\n",
       " 'our': 259,\n",
       " 'ernie': 7514,\n",
       " 'prehistoric': 9780,\n",
       " 'alfred': 4824,\n",
       " 'locker': 9526,\n",
       " 'karl': 5167,\n",
       " 'screwed': 5768,\n",
       " 'square': 4524,\n",
       " 'subway': 6410,\n",
       " 'slowly': 1334,\n",
       " 'pregnant': 2719,\n",
       " 'lightly': 8946,\n",
       " 'staple': 9957,\n",
       " 'affair': 1561,\n",
       " 'springer': 5308,\n",
       " 'smitten': 9508,\n",
       " 'homosexuality': 6185,\n",
       " 'cannibals': 8346,\n",
       " 'establishing': 6690,\n",
       " 'denis': 5579,\n",
       " 'macy': 4201,\n",
       " 'mormon': 6676,\n",
       " 'cute': 1009,\n",
       " 'dismiss': 7217,\n",
       " 'defined': 4775,\n",
       " 'factors': 6131,\n",
       " 'authenticity': 6180,\n",
       " 'closing': 2714,\n",
       " 'drunken': 3691,\n",
       " 'sequences': 829,\n",
       " 'consistently': 4111,\n",
       " 'environment': 2762,\n",
       " 'zoom': 8812,\n",
       " 'cue': 5319,\n",
       " 'shepherd': 5999,\n",
       " 'incoherent': 3299,\n",
       " 'caused': 2155,\n",
       " 'deadpan': 8857,\n",
       " 'course': 260,\n",
       " 'sooner': 5491,\n",
       " 'following': 1026,\n",
       " 'message': 730,\n",
       " 'opinions': 4681,\n",
       " 'suspected': 6592,\n",
       " 'ruined': 2233,\n",
       " 'adult': 1114,\n",
       " 'youngest': 5427,\n",
       " 'bitter': 2894,\n",
       " 'visions': 5517,\n",
       " 'bonham': 7546,\n",
       " 'faint': 7894,\n",
       " 'performances': 352,\n",
       " 'gives': 404,\n",
       " 'leading': 954,\n",
       " 'trying': 265,\n",
       " 'stands': 1385,\n",
       " 'were': 71,\n",
       " 'receive': 3875,\n",
       " 'mistake': 1296,\n",
       " 'honour': 9527,\n",
       " 'home': 340,\n",
       " 'elisha': 8834,\n",
       " 'kline': 5418,\n",
       " 'purple': 3786,\n",
       " 'authorities': 5843,\n",
       " 'capture': 1820,\n",
       " 'voodoo': 7640,\n",
       " 'intact': 6971,\n",
       " 'sins': 7280,\n",
       " 'steiner': 9970,\n",
       " 'comprehend': 6525,\n",
       " 'angel': 2203,\n",
       " 'quarter': 6412,\n",
       " 'cuts': 1894,\n",
       " 'dick': 1787,\n",
       " 'patient': 2981,\n",
       " 'garbage': 1219,\n",
       " 'presumably': 3487,\n",
       " 'pro': 3218,\n",
       " 'fills': 6659,\n",
       " 'picking': 3597,\n",
       " 'spade': 7528,\n",
       " 'horse': 1767,\n",
       " 'base': 2792,\n",
       " 'illustrates': 9600,\n",
       " 'recommended': 1158,\n",
       " 'corbin': 8487,\n",
       " 'looked': 598,\n",
       " 'monday': 8259,\n",
       " 'occasion': 4065,\n",
       " 'acknowledged': 9495,\n",
       " 'sirk': 4530,\n",
       " 'subsequent': 3644,\n",
       " 'ivy': 9931,\n",
       " 'harris': 2016,\n",
       " 'shirley': 4152,\n",
       " 'local': 702,\n",
       " 'ask': 913,\n",
       " 'startling': 6002,\n",
       " 'batwoman': 9068,\n",
       " 'alicia': 5143,\n",
       " 'health': 3310,\n",
       " 'periods': 7172,\n",
       " 'gory': 2173,\n",
       " 'siblings': 6222,\n",
       " 'superiors': 9350,\n",
       " 'jason': 1613,\n",
       " 'alien': 1458,\n",
       " 'scifi': 6148,\n",
       " 'freeze': 6070,\n",
       " 'godfather': 3362,\n",
       " 'jenny': 4481,\n",
       " 'fascination': 5186,\n",
       " 'glow': 8692,\n",
       " 'lacks': 1488,\n",
       " 'hitch': 9709,\n",
       " 'amoral': 8867,\n",
       " 'measure': 4133,\n",
       " 'optimism': 8562,\n",
       " 'federal': 8489,\n",
       " 'tea': 3341,\n",
       " 'baddie': 8313,\n",
       " 'patriotic': 8703,\n",
       " 't': 24,\n",
       " 'hunter': 2136,\n",
       " 'overlooked': 3462,\n",
       " 'laid': 2959,\n",
       " 'streets': 1929,\n",
       " 'fishburne': 5855,\n",
       " 'britain': 3112,\n",
       " 'inexplicably': 5320,\n",
       " 'greatest': 814,\n",
       " 'wong': 6340,\n",
       " 'pen': 7103,\n",
       " 'unconscious': 8860,\n",
       " 'cape': 6473,\n",
       " 'walsh': 4788,\n",
       " 'comical': 2803,\n",
       " 'better': 129,\n",
       " 'forgiven': 5905,\n",
       " 'flashes': 6384,\n",
       " 'heston': 3333,\n",
       " 'induced': 9031,\n",
       " 'season': 784,\n",
       " 'olympia': 9841,\n",
       " 'settings': 2775,\n",
       " 'winslet': 9875,\n",
       " 'stan': 3189,\n",
       " 'title': 422,\n",
       " 'denying': 8633,\n",
       " 'cheat': 8887,\n",
       " 'profoundly': 8505,\n",
       " 'spelling': 8281,\n",
       " 'linda': 4086,\n",
       " 'mrs': 1988,\n",
       " 'literally': 1205,\n",
       " 'deathtrap': 7621,\n",
       " 'macmurray': 7919,\n",
       " 'ceremony': 8458,\n",
       " 'monologues': 8397,\n",
       " 'unfairly': 9738,\n",
       " 'adequately': 9113,\n",
       " 'grateful': 6023,\n",
       " 'vampire': 1309,\n",
       " 'showtime': 9521,\n",
       " 'reeve': 6106,\n",
       " 'people': 79,\n",
       " 'steve': 1170,\n",
       " 'shots': 650,\n",
       " 'picture': 427,\n",
       " 'zelah': 7872,\n",
       " 'covering': 6811,\n",
       " 'gym': 9399,\n",
       " 'cardboard': 3425,\n",
       " 'hokey': 5592,\n",
       " 'pope': 9950,\n",
       " 'tonight': 4440,\n",
       " 'producing': 3993,\n",
       " 'challenging': 4712,\n",
       " 'sensibilities': 8194,\n",
       " 'profit': 8698,\n",
       " 'approved': 9867,\n",
       " 'orlando': 8333,\n",
       " 'vcr': 7916,\n",
       " 'andy': 1669,\n",
       " 'anyways': 3835,\n",
       " 'gross': 2687,\n",
       " 'irony': 3132,\n",
       " 'longtime': 9351,\n",
       " 'sci': 898,\n",
       " 'surgery': 5769,\n",
       " 'mgm': 2474,\n",
       " 'curly': 5269,\n",
       " 'favorite': 503,\n",
       " 'stargate': 4935,\n",
       " 'thurman': 6411,\n",
       " 'inept': 2755,\n",
       " 'lame': 817,\n",
       " 'useful': 4486,\n",
       " ...}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#word2idx = {}\n",
    "#for idx, word in enumerate(vocab):\n",
    "#    word2idx[word] = idx\n",
    "\n",
    "# dictionary comprehension\n",
    "word2idx = {word: i for i, word in enumerate(vocab)}\n",
    "\n",
    "word2idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text to vector function\n",
    "\n",
    "Now we can write a function that converts a some text to a word vector. The function will take a string of words as input and return a vector with the words counted up. Here's the general algorithm to do this:\n",
    "\n",
    "* Initialize the word vector with [np.zeros](https://docs.scipy.org/doc/numpy/reference/generated/numpy.zeros.html), it should be the length of the vocabulary.\n",
    "* Split the input string of text into a list of words with `.split(' ')`. Again, if you call `.split()` instead, you'll get slightly different results than what we show here.\n",
    "* For each word in that list, increment the element in the index associated with that word, which you get from `word2idx`.\n",
    "\n",
    "**Note:** Since all words aren't in the `vocab` dictionary, you'll get a key error if you run into one of those words. You can use the `.get` method of the `word2idx` dictionary to specify a default returned value when you make a key error. For example, `word2idx.get(word, None)` returns `None` if `word` doesn't exist in the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_vector(text):\n",
    "    vector = np.zeros(len(vocab))\n",
    "    for word in text.split(' '):\n",
    "        if (word2idx.get(word) is None):\n",
    "            continue\n",
    "        else:\n",
    "            vector[word2idx[word]] += 1\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you do this right, the following code should return\n",
    "\n",
    "```\n",
    "text_to_vector('The tea is for a party to celebrate '\n",
    "               'the movie so she has no time for a cake')[:65]\n",
    "                   \n",
    "array([0, 1, 0, 0, 2, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0,\n",
    "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
    "       0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0])\n",
    "```       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  1.,  0.,  0.,  2.,  0.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  2.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,\n",
       "        0.,  0.,  1.,  0.,  0.,  0.,  1.,  1.,  0.,  0.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_to_vector('The tea is for a party to celebrate '\n",
    "               'the movie so she has no time for a cake')[:65]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, run through our entire review data set and convert each review to a word vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = np.zeros((len(reviews), len(vocab)), dtype=np.int_)\n",
    "for ii, (_, text) in enumerate(reviews.iterrows()):\n",
    "    word_vectors[ii] = text_to_vector(text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 18,   9,  27,   1,   4,   4,   6,   4,   0,   2,   2,   5,   0,\n",
       "          4,   1,   0,   2,   0,   0,   0,   0,   0,   0],\n",
       "       [  5,   4,   8,   1,   7,   3,   1,   2,   0,   4,   0,   0,   0,\n",
       "          1,   2,   0,   0,   1,   3,   0,   0,   0,   1],\n",
       "       [ 78,  24,  12,   4,  17,   5,  20,   2,   8,   8,   2,   1,   1,\n",
       "          2,   8,   0,   5,   5,   4,   0,   2,   1,   4],\n",
       "       [167,  53,  23,   0,  22,  23,  13,  14,   8,  10,   8,  12,   9,\n",
       "          4,  11,   2,  11,   5,  11,   0,   5,   3,   0],\n",
       "       [ 19,  10,  11,   4,   6,   2,   2,   5,   0,   1,   2,   3,   1,\n",
       "          0,   0,   0,   3,   1,   0,   1,   0,   0,   0]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Printing out the first 5 word vectors\n",
    "word_vectors[:5, :23]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train, Validation, Test sets\n",
    "\n",
    "Now that we have the word_vectors, we're ready to split our data into train, validation, and test sets. Remember that we train on the train data, use the validation data to set the hyperparameters, and at the very end measure the network performance on the test data. Here we're using the function `to_categorical` from TFLearn to reshape the target data so that we'll have two output units and can classify with a softmax activation function. We actually won't be creating the validation set here, TFLearn will do that for us later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y = (labels=='positive').astype(np.int_)\n",
    "records = len(labels)\n",
    "\n",
    "shuffle = np.arange(records)\n",
    "np.random.shuffle(shuffle)\n",
    "test_fraction = 0.9\n",
    "\n",
    "train_split, test_split = shuffle[:int(records*test_fraction)], shuffle[int(records*test_fraction):]\n",
    "trainX, trainY = word_vectors[train_split,:], to_categorical(Y.values[train_split], 2)\n",
    "testX, testY = word_vectors[test_split,:], to_categorical(Y.values[test_split], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       ..., \n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the network\n",
    "\n",
    "[TFLearn](http://tflearn.org/) lets you build the network by [defining the layers](http://tflearn.org/layers/core/). \n",
    "\n",
    "### Input layer\n",
    "\n",
    "For the input layer, you just need to tell it how many units you have. For example, \n",
    "\n",
    "```\n",
    "net = tflearn.input_data([None, 100])\n",
    "```\n",
    "\n",
    "would create a network with 100 input units. The first element in the list, `None` in this case, sets the batch size. Setting it to `None` here leaves it at the default batch size.\n",
    "\n",
    "The number of inputs to your network needs to match the size of your data. For this example, we're using 10000 element long vectors to encode our input data, so we need 10000 input units.\n",
    "\n",
    "\n",
    "### Adding layers\n",
    "\n",
    "To add new hidden layers, you use \n",
    "\n",
    "```\n",
    "net = tflearn.fully_connected(net, n_units, activation='ReLU')\n",
    "```\n",
    "\n",
    "This adds a fully connected layer where every unit in the previous layer is connected to every unit in this layer. The first argument `net` is the network you created in the `tflearn.input_data` call. It's telling the network to use the output of the previous layer as the input to this layer. You can set the number of units in the layer with `n_units`, and set the activation function with the `activation` keyword. You can keep adding layers to your network by repeated calling `net = tflearn.fully_connected(net, n_units)`.\n",
    "\n",
    "### Output layer\n",
    "\n",
    "The last layer you add is used as the output layer. Therefore, you need to set the number of units to match the target data. In this case we are predicting two classes, positive or negative sentiment. You also need to set the activation function so it's appropriate for your model. Again, we're trying to predict if some input data belongs to one of two classes, so we should use softmax.\n",
    "\n",
    "```\n",
    "net = tflearn.fully_connected(net, 2, activation='softmax')\n",
    "```\n",
    "\n",
    "### Training\n",
    "To set how you train the network, use \n",
    "\n",
    "```\n",
    "net = tflearn.regression(net, optimizer='sgd', learning_rate=0.1, loss='categorical_crossentropy')\n",
    "```\n",
    "\n",
    "Again, this is passing in the network you've been building. The keywords: \n",
    "\n",
    "* `optimizer` sets the training method, here stochastic gradient descent\n",
    "* `learning_rate` is the learning rate\n",
    "* `loss` determines how the network error is calculated. In this example, with the categorical cross-entropy.\n",
    "\n",
    "Finally you put all this together to create the model with `tflearn.DNN(net)`. So it ends up looking something like \n",
    "\n",
    "```\n",
    "net = tflearn.input_data([None, 10])                          # Input\n",
    "net = tflearn.fully_connected(net, 5, activation='ReLU')      # Hidden\n",
    "net = tflearn.fully_connected(net, 2, activation='softmax')   # Output\n",
    "net = tflearn.regression(net, optimizer='sgd', learning_rate=0.1, loss='categorical_crossentropy')\n",
    "model = tflearn.DNN(net)\n",
    "```\n",
    "\n",
    "> **Exercise:** Below in the `build_model()` function, you'll put together the network using TFLearn. You get to choose how many layers to use, how many hidden units, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Network building\n",
    "def build_model():\n",
    "    # This resets all parameters and variables, leave this here\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    #### Your code ####\n",
    "    net = tflearn.input_data([None, len(vocab)])\n",
    "    net = tflearn.fully_connected(net, 200, activation='ReLU')\n",
    "    net = tflearn.fully_connected(net, 25, activation='ReLU')\n",
    "    net = tflearn.fully_connected(net, 2, activation='softmax') \n",
    "    net = tflearn.regression(net, optimizer='sgd', learning_rate=0.1, loss='categorical_crossentropy')\n",
    "    \n",
    "    model = tflearn.DNN(net)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intializing the model\n",
    "\n",
    "Next we need to call the `build_model()` function to actually build the model. In my solution I haven't included any arguments to the function, but you can add arguments so you can change parameters in the model if you want.\n",
    "\n",
    "> **Note:** You might get a bunch of warnings here. TFLearn uses a lot of deprecated code in TensorFlow. Hopefully it gets updated to the new TensorFlow version soon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = build_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the network\n",
    "\n",
    "Now that we've constructed the network, saved as the variable `model`, we can fit it to the data. Here we use the `model.fit` method. You pass in the training features `trainX` and the training targets `trainY`. Below I set `validation_set=0.1` which reserves 10% of the data set as the validation set. You can also set the batch size and number of epochs with the `batch_size` and `n_epoch` keywords, respectively. Below is the code to fit our the network to our word vectors.\n",
    "\n",
    "You can rerun `model.fit` to train the network further if you think you can increase the validation accuracy. Remember, all hyperparameter adjustments must be done using the validation set. **Only use the test set after you're completely done training the network.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1589  | total loss: \u001b[1m\u001b[32m0.57917\u001b[0m\u001b[0m | time: 5.000s\n",
      "| SGD | epoch: 010 | loss: 0.57917 - acc: 0.7196 -- iter: 20224/20250\n",
      "Training Step: 1590  | total loss: \u001b[1m\u001b[32m0.58006\u001b[0m\u001b[0m | time: 6.034s\n",
      "| SGD | epoch: 010 | loss: 0.58006 - acc: 0.7164 | val_loss: 0.50576 - val_acc: 0.7676 -- iter: 20250/20250\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "model.fit(trainX, trainY, validation_set=0.1, show_metric=True, batch_size=128, n_epoch=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "After you're satisified with your hyperparameters, you can run the network on the test set to measure its performance. Remember, *only do this after finalizing the hyperparameters*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.7676\n"
     ]
    }
   ],
   "source": [
    "predictions = (np.array(model.predict(testX))[:,0] >= 0.5).astype(np.int_)\n",
    "test_accuracy = np.mean(predictions == testY[:,0], axis=0)\n",
    "print(\"Test accuracy: \", test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try out your own text!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper function that uses your model to predict sentiment\n",
    "def test_sentence(sentence):\n",
    "    positive_prob = model.predict([text_to_vector(sentence.lower())])[0][1]\n",
    "    print('Sentence: {}'.format(sentence))\n",
    "    print('P(positive) = {:.3f} :'.format(positive_prob), \n",
    "          'Positive' if positive_prob > 0.5 else 'Negative')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: Moonlight is by far the best movie of 2016.\n",
      "P(positive) = 0.705 : Positive\n",
      "Sentence: It's amazing anyone could be talented enough to make something this spectacularly awful\n",
      "P(positive) = 0.416 : Negative\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Moonlight is by far the best movie of 2016.\"\n",
    "test_sentence(sentence)\n",
    "\n",
    "sentence = \"It's amazing anyone could be talented enough to make something this spectacularly awful\"\n",
    "test_sentence(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
